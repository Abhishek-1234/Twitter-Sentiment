{"name":"Twitter-sentiment","tagline":"Sentiment Analysis of Twitter Feeds","body":"SENTIMENT ANALYSIS OF TWITTER FEEDS  \r\n===================================\r\n![Figure](docs/jul14/img/iitd_logo_128.png)  \r\nDEPARTMEMT OF MATHEMATICS  \r\n-------------------------\r\nINDIAN INSTITUTE OF TECHNOLOGY, DELHI  \r\n-------------------------------------  \r\n  \r\n  \r\n  \r\n  \r\n  \r\n  \r\n  \r\n\r\n###  Yogesh Garg  \r\n\r\n###  Dr. Niladri Chatterjee  \r\n\r\n### July, 2014\r\n\r\n\r\n# Contents\r\n\r\n    1  Introduction  \r\n        1.1  Sentiment Analysis  \r\n        1.2  Twitter  \r\n    2  Literature Review  \r\n    3  Methodology  \r\n        3.1  Datasets  \r\n            3.1.1  Twitter Sentiment Corpus  \r\n            3.1.2  Stanford Twitter  \r\n        3.2  Pre Processing  \r\n            3.2.1  Hashtags  \r\n            3.2.2  Handles  \r\n            3.2.3  URLs  \r\n            3.2.4  Emoticons  \r\n            3.2.5  Punctuations  \r\n            3.2.6  Repeating Characters  \r\n        3.3  Stemming Algorithms  \r\n            3.3.1  Porter Stemmer  \r\n            3.3.2  Lemmatization  \r\n        3.4  Features  \r\n            3.4.1  Unigrams  \r\n            3.4.2  N-grams  \r\n            3.4.3  Negation Handling  \r\n    4  Experimentation  \r\n        4.1  Naive Bayes  \r\n        4.2  Maximum Entropy Classifier  \r\n    5  Future Work  \r\n    6  Conclusion  \r\n\r\n# List of Tables\r\n\r\n    1  Twitter Sentiment Corpus  \r\n    2  Stanford Corpus  \r\n    3  Frequency of Features per Tweet  \r\n    4  List of Emoticons  \r\n    5  List of Punctuations  \r\n    6  Number of words before and after pre-processing  \r\n    7  Porter Stemmer Steps  \r\n    8  Explicit Negation Cues  \r\n\r\n# List of Figures\r\n\r\n    1  Schematic Diagram of Methodology  \r\n    2  Illustration of a Tweet with various features  \r\n    3  Cumulative Frequency Plot for 50 Most Frequent Unigrams  \r\n    4  Zipf's Law - Log Frequency versus Log Rank plot for unigrams  \r\n    5  Number of n-grams vs. Number of Tweets  \r\n    6  Number of repeating n-grams vs. Number of Tweets  \r\n    7  Cumulative Frequency Plot for 50 Most Frequent Bigrams  \r\n    8  Cumulative Frequency Plot for 50 Most Frequent Trigrams  \r\n    9  Scope of Negation  \r\n    10  Accuracy for Naive Bayes Classifier  \r\n    11  Precision vs. Recall for Naive Bayes Classifier  \r\n    12  Precision vs. Recall for Maximum Entropy Classifier  \r\n\r\n##  1  Introduction\r\n\r\n###  1.1  Sentiment Analysis\r\n\r\nSentiment Analysis refers to the use of text analysis and natural language\r\nprocessing to identify and extract subjective information in textual contents.\r\nThere are two type of user-generated content available on the web – facts and\r\nopinions. Facts are statements about topics and in the current scenario,\r\neasily collectible from the Internet using search engines that index documents\r\nbased on topic keywords. Opinions are user specific statement exhibiting\r\npositive or negative sentiments about a certain topic. Generally opinions are\r\nhard to categorize using keywords. Various text analysis and machine learning\r\ntechniques are used to mine opinions from a document [1]. Sentiment Analysis\r\nfinds its application in a variety of domains.\r\n\r\n**Business**\r\n     Businesses may use sentiment analysis on blogs, review websites etc. to judge the market response of a product. This information may also be used for intelligent placement of advertisements. For example, if product \"A\" and \"B\" are competitors and an online merchant business \"M\" sells both, then \"M\" may advertise for \"A\" if the user displays positive sentiments towards \"A\", its brand or related products, or \"B\" if they display negative sentiments towards \"A\". \r\n**Government**\r\n     Governments and politicians can actively monitor public sentiments as a response to their current policies, speeches made during campaigns etc. This will help them make create better public awareness regarding policies and even drive campaigns intelligently. \r\n**Financial Markets**\r\n     Public opinion regarding companies can be used to predict performance of their stocks in the financial markets. If people have a positive opinion about a product that a company A has launched, then the share prices of A are likely to go higher and vice versa. Public opinion can be used as an additional feature in existing models that try to predict market performances based on historical data. \r\n\r\n###  1.2  Twitter\r\n\r\nTwitter is an online social networking and micro-blogging service that enables\r\nusers to create and read short messages, called \"Tweets\". It is a global forum\r\nwith the presence of eminent personalities from the field of entertainment,\r\nindustry and politics. People tweet about their life, events and express\r\nopinion about various topics using text messages limited to 140 characters.\r\nRegistered users can read and post tweets, but any unregistered users can read\r\nthem. Twitter can be accessed via Web, SMS, or mobile apps. Traditionally a\r\nlarge volume of research in sentiment analysis and opinion mining has been\r\ndirected towards larger pieces of text like movie reviews. Sentiment Analysis\r\nin micro-blogging sphere is relatively new. From the perspective of Sentiment\r\nAnalysis, we discuss a few characteristics of Twitter:\r\n\r\n**Length of a Tweet**\r\n     The maximum length of a Twitter message is 140 characters. This means that we can practically consider a tweet to be a single sentence, void of complex grammatical constructs. This is a vast difference from traditional subjects of Sentiment Analysis, such as movie reviews. \r\n**Language used**\r\n     Twitter is used via a variety of media including SMS and mobile phone apps. Because of this and the 140-character limit, language used in Tweets tend be more colloquial, and filled with slang and misspellings. Use of hashtags also gained popularity on Twitter and is a primary feature in any given tweet. Our analysis shows that there are approximately 1-2 hashtags per tweet, as shown in Table 3 . \r\n**Data availability**\r\n     Another difference is the magnitude of data available. With the Twitter API, it is easy to collect millions of tweets for training. There also exist a few datasets that have automatically and manually labelled the tweets [2] [3]. \r\n**Domain of topics**\r\n     People often post about their likes and dislikes on social media. These are not al concentrated around one topic. This makes twitter a unique place to model a generic classifier as opposed to domain specific classifiers that could be build datasets such as movie reviews. \r\n\r\n##  2  Literature Review\r\n\r\nPang and Lee discuss in detail the current techniques in the area of sentiment\r\nanalysis. Much of it revolves around reviews for movies and products.\r\n\r\nGo, Bhayani and Huang (2009) were among the first to explore sentiment\r\nanalysis on Twitter [2]. They classify Tweets for a query term into negative\r\nor positive sentiment. They collect training dataset automatically from\r\nTwitter. To collect positive and negative tweets, they query twitter for happy\r\nand sad emoticons. Happy emoticons are different versions of smiling face,\r\nlike \"`:)`\", \"`:-)`\", \"`: )`\", \"`:D`\", \"`=)`\" etc. Sad emoticons include\r\nfrowns, like \"`:(`\", \"`:-(`\", \"`:(`\" etc. They try various features –\r\nunigrams, bigrams and Part-of-Speech and train their classifier on various\r\nmachine learning algorithms – Naive Bayes, Maximum Entropy and Scalable Vector\r\nMachines and compare it against a baseline classifier by counting the number\r\nof positive and negative words from a publicly available corpus. They report\r\nthat Bigrams alone and Part-of-Speech Tagging are not helpful and that Naive\r\nBayes Classifier gives the best results.\r\n\r\nPak and Paroubek (2010) use a similar distant supervision technique to\r\nautomatically collect the dataset from the web [4]. Apart from using happy and\r\nsad emoticons for positive and negative sentiment respectively, they also\r\nquery tweets from accounts of 44 newspapers, like “New York Times”,\r\n“Washington Posts” etc. to collect a training set of subjective tweets. They\r\nuse unigrams and filtered n-grams for their classification. They also handle\r\nnegations by attaching negative cues, like “no”, “not” to the words preceding\r\nand following them. They report that both bigrams and negation handling help.\r\n\r\nKoulompis, Wilson and Moore (2011) identify that use of informal and creative\r\nlanguage make sentiment analysis of tweets a rather different task [5]. They\r\nleverage previous work done in hashtags and sentiment analysis to build their\r\nclassifier. They use Edinburgh Twitter corpus to find out most frequent\r\nhashtags. They manually classify these hashtags and use them to in turn\r\nclassify the tweets. Apart from using n-grams and Part-of-Speech features,\r\nthey also build a feature set from already existing MPQA subjectivity lexicon\r\nand Internet Lingo Dictionary. They report that the best results are seen with\r\nn-gram features with lexicon features, while using Part-of-Speech features\r\ncauses a drop in accuracy.\r\n\r\nSaif, He and Alani (2012) discuss a semantic based approach to identify the\r\nentity being discussed in a tweet, like a person, organization etc. [6]. They\r\nalso demonstrate that removal of stop words is not a necessary step and may\r\nhave undesirable effect on the classifier.\r\n\r\nAll of the aforementioned techniques rely on n-gram features. It is unclear\r\nthat the use of Part-of-Speech tagging is useful or not. To improve accuracy,\r\nsome employ different methods of feature selection or leveraging knowledge\r\nabout micro-blogging. In contrast, we improve our results by using more basic\r\ntechniques used in Sentiment Analysis, like stemming, two-step classification\r\nand negation detection and scope of negation.\r\n\r\nNegation detection is a technique that has often been studied in sentiment\r\nanalysis. Negation words like “not”, “never”, “no” etc. can drastically change\r\nthe meaning of a sentence and hence the sentiment expressed in them. Due to\r\npresence of such words, the meaning of nearby words becomes opposite. Such\r\nwords are said to be in the scope of negation. Many researches have worked on\r\ndetecting the scope of negation.\r\n\r\nThe scope of negation of a cue can be taken from that word to the next\r\nfollowing punctuation. Councill, McDonald and Velikovich (2010) discuss a\r\ntechnique to identify negation cues and their scope in a sentence [7]. They\r\nidentify explicit negation cues in the text and for each word in the scope.\r\nThen they find its distance from the nearest negative cue on the left and\r\nright.\r\n\r\n##  3  Methodology\r\n\r\nWe use different feature sets and machine learning classifiers to determine\r\nthe best combination for sentiment analysis of twitter. We also experiment\r\nwith various pre-processing steps like - punctuations, emoticons, twitter\r\nspecific terms and stemming. We investigated the following features -\r\nunigrams, bigrams, trigrams and negation detection. We finally train our\r\nclassifier using various machine-learning algorithms - Naive Bayes, Decision\r\nTrees and Maximum Entropy.\r\n\r\n!![Figure](docs/jul14/img/schematic.png)\r\n\r\nFigure 1: Schematic Diagram of Methodology\r\n\r\nWe use a modularized approach with feature extractor and classification\r\nalgorithm as two independent components. This enables us to experiment with\r\ndifferent options for each component. Figure 1 illustrates different steps\r\ntaken in the entire process.\r\n\r\n###  3.1  Datasets\r\n\r\nOne of the major challenges in Sentiment Analysis of Twitter is to collect a\r\nlabelled dataset. Researchers have made public the following datasets for\r\ntraining and testing classifiers.\r\n\r\n####  3.1.1  Twitter Sentiment Corpus\r\n\r\nThis is a collection of 5513 tweets collected for four different topics,\r\nnamely, Apple, Google, Microsoft, Twitter It is collected and hand-classified\r\nby Sanders Analytics LLC [3]. Each entry in the corpus contains, Tweet id,\r\nTopic and a Sentiment label. We use Twitter-Python library to enrich this data\r\nby downloading data like Tweet text, Creation Date, Creator etc. for every\r\nTweet id. Each Tweet is hand classified by an American male into the following\r\nfour categories. For the purpose of our experiments, we consider Irrelevant\r\nand Neutral to be the same class. Illustration of Tweets in this corpus is\r\nshow in Table 1 .\r\n\r\n**Positive**\r\n     For showing positive sentiment towards the topic\r\n**Positive**\r\n     For showing no or mixed or weak sentiments towards the topic\r\n**Negative**\r\n     For showing negative sentiment towards the topic\r\n**Irrelevant**\r\n     For non English text or off-topic comments\r\n\r\n<div style=\"text-align:center\">\r\n<table border=\"1\">\r\n<tr><td align=\"left\">Class </td><td align=\"right\">Count </td><td width=\"0\">Example </td></tr>\r\n<tr><td align=\"left\">neg </td><td align=\"right\">529 </td><td width=\"0\">#Skype often crashing: #microsoft, what are you doing? </td></tr>\r\n<tr><td align=\"left\">neu </td><td align=\"right\">3770 </td><td width=\"0\">How #Google Ventures Chooses Which Startups Get Its $200\r\n                Million http://t.co/FCWXoUd8 via @mashbusiness @mashable </td></tr>\r\n<tr><td align=\"left\">pos </td><td align=\"right\">483 </td><td width=\"0\">Now all @Apple has to do is get swype on the iphone and\r\n                it will be crack. Iphone that is </td></tr></table>\r\n\r\n\r\n<div class=\"p\"><!----></div>\r\n<div style=\"text-align:center\">Table 1: Twitter Sentiment Corpus</div>\r\n<a id=\"tab:TSC\">\r\n</a>\r\n</div>\r\n\r\n####  3.1.2  Stanford Twitter\r\n\r\nThis corpus of tweets, developed by Sanford’s Natural Language processing\r\nresearch group, is publically available [2]. The training set is collected by\r\nquerying Twitter API for happy emoticons like \"`:)`\" and sad emoticons like\r\n\"`:(`\" and labelling them positive or negative. The emoticons were then\r\nstripped and Re-Tweets and duplicates removed. It also contains around 500\r\ntweets manually collected and labelled for testing purposes. We randomly\r\nsample and use 5000 tweets from this dataset. An example of Tweets in this\r\ncorpus are shown in Table 2 .\r\n\r\n<div style=\"text-align:center\">\r\n<table border=\"1\">\r\n<tr><td align=\"left\">Class </td><td align=\"right\">Count </td><td width=\"0\">Example </td></tr>\r\n<tr><td align=\"left\">neg </td><td align=\"right\">2501 </td><td width=\"0\">Playing after the others thanks to TV scheduling may well allow us to know what's go on, but it makes things look bad on Saturday nights  </td></tr>\r\n<tr><td align=\"left\">pos </td><td align=\"right\">2499 </td><td width=\"0\">@francescazurlo HAHA!!! how long have you been singing that song now? It has to be at least a day. i think you're wildly entertaining!  </td></tr></table>\r\n\r\n\r\n<div class=\"p\"><!----></div>\r\n<div style=\"text-align:center\">Table 2: Stanford Corpus</div>\r\n<a id=\"tab:STAN\">\r\n</a>\r\n</div>\r\n\r\n###  3.2  Pre Processing\r\n\r\nUser-generated content on the web is seldom present in a form usable for\r\nlearning. It becomes important to normalize the text by applying a series of\r\npre-processing steps. We have applied an extensive set of pre-processing steps\r\nto decrease the size of the feature set to make it suitable for learning\r\nalgorithms. Figure 2 illustrates various features seen in micro-blogging.\r\nTable 3 illustrates the frequency of these features per tweet, cut by\r\ndatasets. We also give a brief description of pre-processing steps taken.\r\n\r\n![Figure](docs/jul14/img/tweet.png)\r\n\r\nFigure 2: Illustration of a Tweet with various features\r\n\r\n<div style=\"text-align:center\">\r\n<table border=\"1\">\r\n<tr><td align=\"left\"></td><td colspan=\"2\" align=\"center\">Twitter Sentiment\r\n </td><td colspan=\"2\" align=\"center\">Stanford Corpus\r\n </td><td colspan=\"2\" align=\"center\">Both </td></tr>\r\n<tr><td align=\"left\">Features   </td><td colspan=\"1\" align=\"center\">Avg. </td><td colspan=\"1\" align=\"center\">Max.\r\n            </td><td colspan=\"1\" align=\"center\">Avg. </td><td colspan=\"1\" align=\"center\">Max.\r\n            </td><td colspan=\"1\" align=\"center\">Avg. </td><td colspan=\"1\" align=\"center\">Max. </td></tr>\r\n<tr><td align=\"left\">Handles        </td><td align=\"right\">0.6761 </td><td align=\"right\">8 </td><td align=\"right\">0.4888 </td><td align=\"right\">10 </td><td align=\"right\">0.5804 </td><td align=\"right\">10 </td></tr>\r\n<tr><td align=\"left\">Hashtags   </td><td align=\"right\">2.0276 </td><td align=\"right\">13 </td><td align=\"right\">0.0282 </td><td align=\"right\">11 </td><td align=\"right\">1.0056 </td><td align=\"right\">13 </td></tr>\r\n<tr><td align=\"left\">Urls       </td><td align=\"right\">0.4431 </td><td align=\"right\">4 </td><td align=\"right\">0.0452 </td><td align=\"right\">2 </td><td align=\"right\">0.2397 </td><td align=\"right\">4 </td></tr>\r\n<tr><td align=\"left\">Emoticons  </td><td align=\"right\">0.0550 </td><td align=\"right\">3 </td><td align=\"right\">0.0154 </td><td align=\"right\">4 </td><td align=\"right\">0.0348 </td><td align=\"right\">4 </td></tr>\r\n<tr><td align=\"left\">Words      </td><td align=\"right\">14.4084 </td><td align=\"right\">31 </td><td align=\"right\">13.2056 </td><td align=\"right\">33 </td><td align=\"right\">13.7936 </td><td align=\"right\">33 </td></tr></table>\r\n\r\n\r\n<div class=\"p\"><!----></div>\r\n<div style=\"text-align:center\">Table 3: Frequency of Features per Tweet</div>\r\n<a id=\"tab:feat_freq\">\r\n</a>\r\n</div>\r\n\r\n####  3.2.1  Hashtags\r\n\r\nA hashtag is a word or an un-spaced phrase prefixed with the hash symbol (#).\r\nThese are used to both naming subjects and phrases that are currently in\r\ntrending topics. For example, #iPad, #news\r\n\r\nRegular Expression: `#(\\w+)`\r\n\r\nReplace Expression: `HASH_\\1`\r\n\r\n####  3.2.2  Handles\r\n\r\nEvery Twitter user has a unique username. Any thing directed towards that user\r\ncan be indicated be writing their username preceded by ‘@’. Thus, these are\r\nlike proper nouns. For example, @Apple\r\n\r\nRegular Expression: `@(\\w+)`\r\n\r\nReplace Expression: `HNDL_\\1`\r\n\r\n####  3.2.3  URLs\r\n\r\nUsers often share hyperlinks in their tweets. Twitter shortens them using its\r\nin-house URL shortening service, like http://t.co/FCWXoUd8 - such links also\r\nenables Twitter to alert users if the link leads out of its domain. From the\r\npoint of view of text classification, a particular URL is not important.\r\nHowever, presence of a URL can be an important feature. Regular expression for\r\ndetecting a URL is fairly complex because of different types of URLs that can\r\nbe there, but because of Twitter’s shortening service, we can use a relatively\r\nsimple regular expression.\r\n\r\nRegular Expression: `(http|https|ftp)://[a-zA-Z0-9\\\\./]+`\r\n\r\nReplace Expression: `URL`\r\n\r\n####  3.2.4  Emoticons\r\n\r\nUse of emoticons is very prevalent throughout the web, more so on micro-\r\nblogging sites. We identify the following emoticons and replace them with a\r\nsingle word. Table 4 lists the emoticons we are currently detecting. All other\r\nemoticons would be ignored.\r\n\r\n<div style=\"text-align:center\"> \r\n<table border=\"1\">\r\n<tr><td colspan=\"1\" align=\"center\">Emoticons </td><td colspan=\"6\" align=\"center\">Examples </td></tr>\r\n<tr><td align=\"left\"><tt>EMOT_SMILEY</tt>   </td><td align=\"left\"><tt>:-)</tt>  </td><td align=\"left\"><tt>:)</tt>   </td><td align=\"left\"><tt>(:</tt>   </td><td align=\"left\"><tt>(-:</tt>  </td><td align=\"left\"><tt></tt>     </td><td align=\"left\"><tt></tt> </td></tr>\r\n<tr><td align=\"left\"><tt>EMOT_LAUGH</tt>    </td><td align=\"left\"><tt>:-D</tt>  </td><td align=\"left\"><tt>:D</tt>   </td><td align=\"left\"><tt>X-D</tt>  </td><td align=\"left\"><tt>XD</tt>   </td><td align=\"left\"><tt>xD</tt>   </td><td align=\"left\"><tt></tt> </td></tr>\r\n<tr><td align=\"left\"><tt>EMOT_LOVE</tt>     </td><td align=\"left\"><tt>&lt;3</tt>    </td><td align=\"left\"><tt>:*</tt>   </td><td align=\"left\"><tt></tt>     </td><td align=\"left\"><tt></tt>     </td><td align=\"left\"><tt></tt>     </td><td align=\"left\"><tt></tt> </td></tr>\r\n<tr><td align=\"left\"><tt>EMOT_WINK</tt>     </td><td align=\"left\"><tt>;-)</tt>  </td><td align=\"left\"><tt>;)</tt>   </td><td align=\"left\"><tt>;-D</tt>  </td><td align=\"left\"><tt>;D</tt>   </td><td align=\"left\"><tt>(;</tt>   </td><td align=\"left\"><tt>(-;</tt> </td></tr>\r\n<tr><td align=\"left\"><tt>EMOT_FROWN</tt>    </td><td align=\"left\"><tt>:-(</tt>  </td><td align=\"left\"><tt>:(</tt>   </td><td align=\"left\"><tt>(:</tt>   </td><td align=\"left\"><tt>(-:</tt>  </td><td align=\"left\"><tt></tt>     </td><td align=\"left\"><tt></tt> </td></tr>\r\n<tr><td align=\"left\"><tt>EMOT_CRY</tt>  </td><td align=\"left\"><tt>:,(</tt>  </td><td align=\"left\"><tt>:'(</tt>  </td><td align=\"left\"><tt>:\"(</tt>  </td><td align=\"left\"><tt>:((</tt>  </td><td align=\"left\"><tt></tt>     </td><td align=\"left\"><tt></tt> </td></tr></table>\r\n\r\n\r\n<div style=\"text-align:center\">Table 4: List of Emoticons</div>\r\n<a id=\"tab:emot\">\r\n</a>\r\n</div>\r\n\r\n####  3.2.5  Punctuations\r\n\r\nAlthough not all Punctuations are important from the point of view of\r\nclassification but some of these, like question mark, exclamation mark can\r\nalso provide information about the sentiments of the text. We replace every\r\nword boundary by a list of relevant punctuations present at that point. Table\r\n5 lists the punctuations currently identified. We also remove any single\r\nquotes that might exist in the text.\r\n\r\n<div style=\"text-align:center\"> \r\n<table border=\"1\">\r\n<tr><td colspan=\"1\" align=\"center\">Punctuations </td><td colspan=\"2\" align=\"center\">Examples </td></tr>\r\n<tr><td align=\"left\"><tt>PUNC_DOT</tt> </td><td align=\"left\"><tt>.</tt> </td><td align=\"left\"><tt></tt> </td></tr>\r\n<tr><td align=\"left\"><tt>PUNC_EXCL</tt> </td><td align=\"left\"><tt>!</tt> </td><td align=\"left\"><tt>¡</tt> </td></tr>\r\n<tr><td align=\"left\"><tt>PUNC_QUES</tt> </td><td align=\"left\"><tt>?</tt> </td><td align=\"left\"><tt>¿</tt> </td></tr>\r\n<tr><td align=\"left\"><tt>PUNC_ELLP</tt> </td><td align=\"left\"><tt>...</tt> </td><td align=\"left\"><tt>…</tt> </td></tr></table>\r\n\r\n\r\n<div style=\"text-align:center\">Table 5: List of Punctuations</div>\r\n<a id=\"tab:punc\">\r\n</a>\r\n</div>\r\n\r\n####  3.2.6  Repeating Characters\r\n\r\nPeople often use repeating characters while using colloquial language, like\r\n\"I’m in a hurrryyyyy\", \"We won, yaaayyyyy!\" As our final pre-processing step,\r\nwe replace characters repeating more than twice as two characters.\r\n\r\nRegular Expression: `(.)\\1{1,}`\r\n\r\nReplace Expression: `\\1\\1`\r\n\r\n#### Reduction in feature space\r\n\r\nIt’s important to note that by applying these pre-processing steps, we are\r\nreducing our feature set otherwise it can be too sparse. Table 6 lists the\r\ndecrease in feature set due to processing each of these features.\r\n\r\n<div style=\"text-align:center\">\r\n<table border=\"1\">\r\n<tr><td align=\"left\"></td><td colspan=\"2\" align=\"center\">Twitter Sentiment\r\n </td><td colspan=\"2\" align=\"center\">Stanford Corpus\r\n </td><td colspan=\"2\" align=\"center\">Both </td></tr>\r\n<tr><td align=\"left\">Preprocessing\r\n            </td><td colspan=\"1\" align=\"center\">Words </td><td colspan=\"1\" align=\"center\">Percentage\r\n            </td><td colspan=\"1\" align=\"center\">Words </td><td colspan=\"1\" align=\"center\">Percentage\r\n            </td><td colspan=\"1\" align=\"center\">Words </td><td colspan=\"1\" align=\"center\">Percentage </td></tr>\r\n<tr><td align=\"left\">None           </td><td align=\"right\">19128 </td><td align=\"right\"></td><td align=\"right\">15910 </td><td align=\"right\"></td><td align=\"right\">31832 </td><td align=\"right\"></td></tr>\r\n<tr><td align=\"left\">Hashtags       </td><td align=\"right\">18649 </td><td align=\"right\">97.50% </td><td align=\"right\">15550 </td><td align=\"right\">97.74% </td><td align=\"right\">31223 </td><td align=\"right\">98.09% </td></tr>\r\n<tr><td align=\"left\">Handles            </td><td align=\"right\">17118 </td><td align=\"right\">89.49% </td><td align=\"right\">13245 </td><td align=\"right\">83.25% </td><td align=\"right\">27383 </td><td align=\"right\">86.02% </td></tr>\r\n<tr><td align=\"left\">Urls           </td><td align=\"right\">16723 </td><td align=\"right\">87.43% </td><td align=\"right\">15335 </td><td align=\"right\">96.39% </td><td align=\"right\">29083 </td><td align=\"right\">91.36% </td></tr>\r\n<tr><td align=\"left\">Emoticons      </td><td align=\"right\">18631 </td><td align=\"right\">97.40% </td><td align=\"right\">15541 </td><td align=\"right\">97.68% </td><td align=\"right\">31197 </td><td align=\"right\">98.01% </td></tr>\r\n<tr><td align=\"left\">Punctuations   </td><td align=\"right\">13724 </td><td align=\"right\">71.75% </td><td align=\"right\">11225 </td><td align=\"right\">70.55% </td><td align=\"right\">22095 </td><td align=\"right\">69.41% </td></tr>\r\n<tr><td align=\"left\">Repeatings     </td><td align=\"right\">18540 </td><td align=\"right\">96.93% </td><td align=\"right\">15276 </td><td align=\"right\">96.02% </td><td align=\"right\">30818 </td><td align=\"right\">96.81% </td></tr>\r\n<tr><td align=\"left\">All                </td><td align=\"right\">11108 </td><td align=\"right\">58.07% </td><td align=\"right\">8646 </td><td align=\"right\">54.34% </td><td align=\"right\">16981 </td><td align=\"right\">53.35% </td></tr></table>\r\n\r\n\r\n<div class=\"p\"><!----></div>\r\n<div style=\"text-align:center\">Table 6: Number of words before and after pre-processing</div>\r\n<a id=\"tab:reduction\">\r\n</a>\r\n</div>\r\n\r\n###  3.3  Stemming Algorithms\r\n\r\nAll stemming algorithms are of the following major types – affix removing,\r\nstatistical and mixed. The first kind, Affix removal stemmer, is the most\r\nbasic one. These apply a set of transformation rules to each word in an\r\nattempt to cut off commonly known prefixes and / or suffixes [8]. A trivial\r\nstemming algorithm would be to truncate words at N-th symbol. But this\r\nobviously is not well suited for practical purposes.\r\n\r\nJ.B. Lovins described first stemming algorithm in 1968. It defines 294\r\nendings, each linked to one of 29 conditions, plus 35 transformation rules.\r\nFor a word being stemmed, an ending with a satisfying condition is found and\r\nremoved. Another famous stemmer used extensively is described in the next\r\nsection.\r\n\r\n####  3.3.1  Porter Stemmer\r\n\r\nMartin Porter wrote a stemmer that was published in July 1980. This stemmer\r\nwas very widely used and became and remains the de facto standard algorithm\r\nused for English stemming. It offers excellent trade-off between speed,\r\nreadability, and accuracy. It uses a set of around 60 rules applied in 6\r\nsuccessive steps [9]. An important feature to note is that it doesn’t involve\r\nrecursion. The steps in the algorithm are described in Table 7 .\r\n\r\n<div style=\"text-align:center\">\r\n<table border=\"1\">\r\n<tr><td align=\"right\">1.    </td><td align=\"left\">Gets rid of plurals and -ed or -ing suffixes </td></tr>\r\n<tr><td align=\"right\">2.    </td><td align=\"left\">Turns terminal y to i when there is another vowel in the stem￼ </td></tr>\r\n<tr><td align=\"right\">3.    </td><td align=\"left\">Maps double suffixes to single ones: -ization, -ational, etc. </td></tr>\r\n<tr><td align=\"right\">4.    </td><td align=\"left\">Deals with suffixes, -full, -ness etc. </td></tr>\r\n<tr><td align=\"right\">5.￼   </td><td align=\"left\">Takes off -ant, -ence, etc. </td></tr>\r\n<tr><td align=\"right\">6.    </td><td align=\"left\">Removes a final –e </td></tr></table>\r\n\r\n\r\n<div style=\"text-align:center\">Table 7: Porter Stemmer Steps</div>\r\n<a id=\"tab:porter\">\r\n</a>\r\n</div>\r\n\r\n####  3.3.2  Lemmatization\r\n\r\nLemmatization is the process of normalizing a word rather than just finding\r\nits stem. In the process, a suffix may not only be removed, but may also be\r\nsubstituted with a different one. It may also involve first determining the\r\npart-of-speech for a word and then applying normalization rules. It might also\r\ninvolve dictionary look-up. For example, verb ‘saw’ would be lemmatized to\r\n‘see’ and the noun ‘saw’ will remain ‘saw’. For our purpose of classifying\r\ntext, stemming should suffice.\r\n\r\n###  3.4  Features\r\n\r\nA wide variety of features can be used to build a classifier for tweets. The\r\nmost widely used and basic feature set is word n-grams. However, there's a lot\r\nof domain specific information present in tweets that can also be used for\r\nclassifying them. We have experimented with two sets of features:\r\n\r\n####  3.4.1  Unigrams\r\n\r\nUnigrams are the simplest features that can be used for text classification. A\r\nTweet can be represented by a multiset of words present in it. We, however,\r\nhave used the presence of unigrams in a tweet as a feature set. Presence of a\r\nword is more important than how many times it is repeated. Pang et al. found\r\nthat presence of unigrams yields better results than repetition [1]. This also\r\nhelps us to avoid having to scale the data, which can considerably decrease\r\ntraining time [2]. Figure 3 illustrated the cumulative distribution of words\r\nin our dataset.\r\n\r\n![Figure](docs/jul14/img/1grams.png)\r\n\r\nFigure 3: Cumulative Frequency Plot for 50 Most Frequent Unigrams\r\n\r\nWe also observe that the unigrams nicely follow Zipf’s law. It states that in\r\na corpus of natural language, the frequency of any word is inversely\r\nproportional to its rank in the frequency table. Figure 4 is a plot of log\r\nfrequency versus log rank of our dataset. A linear trendline fits well with\r\nthe data.\r\n\r\n  \r\n\r\nlog( f ) = −0.9799 log( r ) + 3.9838\r\n\r\n(1)\r\n\r\n  \r\n\r\nf = 103.9838 r−0.9799\r\n\r\n(2)\r\n\r\n  \r\n\r\nf ∝\r\n\r\n1\r\n\r\n* * *\r\n\r\nr  \r\n\r\n(3)\r\n\r\n![Figure](docs/jul14/img/zipfs_law.png)\r\n\r\nFigure 4: Zipf's Law - Log Frequency versus Log Rank plot for unigrams\r\n\r\n####  3.4.2  N-grams\r\n\r\nN-gram refers to an n-long sequence of words. Probabilistic Language Models\r\nbased on Unigrams, Bigrams and Trigrams can be successfully used to predict\r\nthe next word given a current context of words. In the domain of sentiment\r\nanalysis, the performance of N-grams is unclear. According to Pang et al.,\r\nsome researchers report that unigrams alone are better than bigrams for\r\nclassification movie reviews, while some others report that bigrams and\r\ntrigrams yield better product-review polarity classification [1].\r\n\r\nAs the order of the n-grams increases, they tend to be more and more sparse.\r\nBased on our experiments, we find that number of bigrams and trigrams increase\r\nmuch more rapidly than the number of unigrams with the number of Tweets.\r\nFigure 5 shows the number of n-grams versus number of Tweets. We can observe\r\nthat bigrams and trigrams increase almost linearly where as unigrams are\r\nincreasing logarithmically.\r\n\r\n![Figure](docs/jul14/img/Ngrams_dist_1.png)\r\n\r\nFigure 5: Number of n-grams vs. Number of Tweets\r\n\r\nBecause higher order n-grams are sparsely populated, we decide to trim off the\r\nn-grams that are not seen more than once in the training corpus, because\r\nchances are that these n-grams are not good indicators of sentiments. After\r\nthe filtering out non-repeating n-grams, we see that the number of n-grams is\r\nconsiderably decreased and equals the order of unigrams, as shown in Figure 6\r\n.\r\n\r\n![Figure](docs/jul14/img/Ngrams_dist_2.png)\r\n\r\nFigure 6: Number of repeating n-grams vs. Number of Tweets\r\n\r\nFigure 7 and Figure 8 show the cumulative distribution of the most frequent\r\nbigrams and trigrams respectively.\r\n\r\n![Figure](docs/jul14/img/2grams.png)\r\n\r\nFigure 7: Cumulative Frequency Plot for 50 Most Frequent Bigrams\r\n\r\n![Figure](docs/jul14/img/3grams.png)\r\n\r\nFigure 8: Cumulative Frequency Plot for 50 Most Frequent Trigrams\r\n\r\n####  3.4.3  Negation Handling\r\n\r\nThe need negation detection in sentiment analysis can be illustrated by the\r\ndifference in the meaning of the phrases, \"This is good\" vs. \"This is not\r\ngood\" However, the negations occurring in natural language are seldom so\r\nsimple. Handling the negation consists of two tasks – Detection of explicit\r\nnegation cues and the scope of negation of these words.\r\n\r\nCouncill et al. look at whether negation detection is useful for sentiment\r\nanalysis and also to what extent is it possible to determine the exact scope\r\nof a negation in the text [7]. They describe a method for negation detection\r\nbased on Left and Right Distances of a token to the nearest explicit negation\r\ncue.\r\n\r\n#### Detection of Explicit Negation Cues\r\n\r\nTo detect explicit negation cues, we are looking for the following words in\r\nTable 8 . The search is done using regular expressions.\r\n\r\n<div style=\"text-align:center\">\r\n<table border=\"1\">\r\n<tr><td align=\"left\">S.No. </td><td align=\"left\">Negation Cues </td></tr>\r\n<tr><td align=\"left\">1.  </td><td align=\"left\">never </td></tr>\r\n<tr><td align=\"left\">2.  </td><td align=\"left\">no </td></tr>\r\n<tr><td align=\"left\">3.  </td><td align=\"left\">nothing </td></tr>\r\n<tr><td align=\"left\">4.  </td><td align=\"left\">nowhere </td></tr>\r\n<tr><td align=\"left\">5.  </td><td align=\"left\">noone </td></tr>\r\n<tr><td align=\"left\">6.  </td><td align=\"left\">none </td></tr>\r\n<tr><td align=\"left\">7.  </td><td align=\"left\">not </td></tr>\r\n<tr><td align=\"left\">8.  </td><td align=\"left\">havent </td></tr>\r\n<tr><td align=\"left\">9.  </td><td align=\"left\">hasnt </td></tr>\r\n<tr><td align=\"left\">10.  </td><td align=\"left\">hadnt </td></tr>\r\n<tr><td align=\"left\">11.  </td><td align=\"left\">cant </td></tr>\r\n<tr><td align=\"left\">12.  </td><td align=\"left\">couldnt </td></tr>\r\n<tr><td align=\"left\">13.  </td><td align=\"left\">shouldnt </td></tr>\r\n<tr><td align=\"left\">14.  </td><td align=\"left\">wont </td></tr>\r\n<tr><td align=\"left\">15.  </td><td align=\"left\">wouldnt </td></tr>\r\n<tr><td align=\"left\">16.  </td><td align=\"left\">dont </td></tr>\r\n<tr><td align=\"left\">17.  </td><td align=\"left\">doesnt </td></tr>\r\n<tr><td align=\"left\">18.  </td><td align=\"left\">didnt </td></tr>\r\n<tr><td align=\"left\">19.  </td><td align=\"left\">isnt </td></tr>\r\n<tr><td align=\"left\">20.  </td><td align=\"left\">arent </td></tr>\r\n<tr><td align=\"left\">21.  </td><td align=\"left\">aint </td></tr>\r\n<tr><td align=\"left\">22.  </td><td align=\"left\">Anything ending with \"n't\" </td></tr></table>\r\n\r\n\r\n<div style=\"text-align:center\">Table 8: Explicit Negation Cues</div>\r\n<a id=\"tab:negation\">\r\n</a>\r\n</div>\r\n\r\n#### Scope of Negation\r\n\r\nWords immediately preceding and following the negation cues are the most\r\nnegative and the words that come farther away do not lie in the scope of\r\nnegation of such cues. We define left and right negativity of a word as the\r\nchances that meaning of that word is actually the opposite. Left negativity\r\ndepends on the closest negation cue on the left and similarly for Right\r\nnegativity. Figure 9 illustrates the left and right negativity of words in a\r\ntweet.\r\n\r\n![Figure](docs/jul14/img/negation.png)\r\n\r\nFigure 9: Scope of Negation\r\n\r\n##  4  Experimentation\r\n\r\nWe train 90% of our data using different combinations of features and test\r\nthem on the remaining 10%. We take the features in the following combinations\r\n- only unigrams, unigrams + filtered bigrams and trigrams, unigrams +\r\nnegation, unigrams + filtered bigrams and trigrams + negation. We then train\r\nclassifiers using different classification algorithms - Naive Bayes Classifier\r\nand Maximum Entropy Classifier.\r\n\r\nThe task of classification of a tweet can be done in two steps - first,\r\nclassifying \"neutral\" (or \"subjective\") vs. \"objective\" tweets and second,\r\nclassifying objective tweets into \"positive\" vs. \"negative\" tweets. We also\r\ntrained 2 step classifiers. The accuracies for each of these configuration are\r\nshown in Figure 10 , we discuss these in detail below.\r\n\r\n![Figure](docs/jul14/img/NBME_Accuracy.png)\r\n\r\nFigure 10: Accuracy for Naive Bayes Classifier\r\n\r\n###  4.1  Naive Bayes\r\n\r\nNaive Bayes classifier is the simplest and the fastest classifier. Many\r\nresearchers [2], [4] claim to have gotten best results using this classifier.\r\n\r\nFor a given tweet, if we need to find the label for it, we find the\r\nprobabilities of all the labels, given that feature and then select the label\r\nwith maximum probability.\r\n\r\n  \r\n\r\nlabelNB : = argmaxlabel P(label|features)\r\n\r\n(4)\r\n\r\nIn order to find the probability for a label, this algorithm first uses the\r\nBayes rule to express P(label - features) in terms of P(label) and P(features\r\n- label) as,\r\n\r\n  \r\n\r\nP(label|features) =\r\n\r\nP(label) * P(features|label)\r\n\r\n* * *\r\n\r\nP(features)  \r\n\r\n(5)\r\n\r\nMaking the `naive' assumption that all the features are independent,\r\n\r\n  \r\n\r\nP(label|features) =\r\n\r\nP(label) * P(f1|label) * ... * P(fn|label)\r\n\r\n* * *\r\n\r\nP(features)  \r\n\r\n(6)\r\n\r\nRather than computing P(featues) explicitly, we can just calculate the\r\ndenominator for each label, and normalize them so they sum to one:\r\n\r\n  \r\n\r\nP(label|features) =\r\n\r\nP(label) * P(f1|label) * ... * P(fn|label)\r\n\r\n* * *\r\n\r\n  \r\n∑  \r\nl  \r\n\r\n( P(l) * P(f1|l) * ... * P(fn|l) )\r\n\r\n(7)\r\n\r\nThe results from training the Naive Bayes classifier are shown below in Figure\r\n10 . The accuracy of Unigrams is the lowest at 79.67%. The accuracy increases\r\nif we also use Negation detection (81.66%) or higher order n-grams (86.68%).\r\nWe see that if we use both Negation detection and higher order n-grams, the\r\naccuracy is marginally less than just using higher order n-grams (85.92%). We\r\ncan also note that accuracies for double step classifier are lesser than those\r\nfor corresponding single step.\r\n\r\nWe have also shown Precision versus Recall values for Naive Bayes classifier\r\ncorresponding to different classes – Negative, Neutral and Positive in Figure\r\n11 . The solid markers show the P-R values for single step classifier and\r\nhollow markers show the affect of using double step classifier. Different\r\npoints are for different feature sets. We can see that both precision as well\r\nas recall values are higher for single step than that for double step.\r\n\r\n![Figure](docs/jul14/img/NB_PvsR.png)\r\n\r\nFigure 11: Precision vs. Recall for Naive Bayes Classifier\r\n\r\n###  4.2  Maximum Entropy Classifier\r\n\r\nThis classifier works by finding a probability distribution that maximizes the\r\nlikelihood of testable data. This probability function is parameterized by\r\nweight vector. The optimal value of which can be found out using the method of\r\nLagrange multipliers.\r\n\r\n  \r\n\r\nP(label|features) =\r\n\r\n  \r\n∑  \r\ni  \r\n\r\nwi fi(label)\r\n\r\n* * *\r\n\r\n  \r\n∑  \r\nl ∈ labels  \r\n\r\n  \r\n∑  \r\ni  \r\n\r\nwi fi(l)\r\n\r\n(8)\r\n\r\nThe results from training the Naive Bayes classifier are shown below in Figure\r\n10 . Accuracies follow a similar trend as compared to Naive Bayes classifier.\r\nUnigram is the lowest at 79.73% and we see an increase for negation detection\r\nat 80.96%. The maximum is achieved with unigrams, bigrams and trigrams at\r\n85.22% closely followed by n-grams and negation at 85.16%. Once again, the\r\naccuracies for double step classifiers are considerably lower.\r\n\r\nPrecision versus Recall map is also shown for maximum entropy classifier in\r\nFigure 12 . Here we see that precision of \"neutral\" class increase by using a\r\ndouble step classifier, but with a considerable decrease in its recall and\r\nslight fall in precision of \"negative\" and \"positive\" classes.\r\n\r\n![Figure](docs/jul14/img/ME_PvsR.png)\r\n\r\nFigure 12: Precision vs. Recall for Maximum Entropy Classifier\r\n\r\n##  5  Future Work\r\n\r\n**Investigating Support Vector Machines**\r\n     Several papers have discussed the results using Support Vector Machines (SVMs) also. The next step would be to test our approach on SVMs. However, Go, Bhayani and Huang have reported that SVMs do not increase the accuracy [2]. \r\n**Building a classifier for Hindi tweets**\r\n     There are many users on Twitter that use primarily Hindi language. The approach discussed here can be used to create a Hindi language sentiment classifier. \r\n**Improving Results using Semantics Analysis**\r\n     Understanding the role of the nouns being talked about can help us better classify a given tweet. For example, \"Skype often crashing: microsoft, what are you doing?\" Here Skype is a product and Microsoft is a company. We can use semantic labellers to achieve this. Such an approach is discussed by Saif, He and Alani [6]. \r\n\r\n##  6  Conclusion\r\n\r\nIn this paper, we create a sentiment classifier for twitter using labelled\r\ndata sets. We also investigate the relevance of using a double step classifier\r\nand negation detection for the purpose of sentiment analysis.\r\n\r\nOur baseline classifier that uses just the unigrams achieves an accuracy of\r\naround 80.00%. Accuracy of the classifier increases if we use negation\r\ndetection or introduce bigrams and trigrams. Thus we can conclude that both\r\nNegation Detection and higher order n-grams are useful for the purpose of text\r\nclassification. However, if we use both n-grams and negation detection, the\r\naccuracy falls marginally. We also note that Single step classifiers out\r\nperform double step classifiers. In general, Naive Bayes Classifier performs\r\nbetter than Maximum Entropy Classifier.\r\n\r\nWe achieve the best accuracy of 86.68% in the case of Unigrams + Bigrams +\r\nTrigrams, trained on Naive Bayes Classifier.\r\n\r\n## References\r\n\r\n[1] Bo Pang and Lillian Lee. Opinion mining and sentiment analysis. _Foundations and trends in information retrieval_, 2(1-2):pages 1-135, 2008. \r\n\r\n[2] Alec Go, Richa Bhayani, and Lei Huang. Twitter sentiment classification using distant supervision. _Processing_, pages 1-6, 2009. \r\n\r\n[3] Niek Sanders. Twitter sentiment corpus. http://www.sananalytics.com/lab/twitter-sentiment/. Sanders Analytics. \r\n\r\n[4] Alexander Pak and Patrick Paroubek. Twitter as a corpus for sentiment analysis and opinion mining. volume 2010, pages 1320-1326, 2010. \r\n\r\n[5] Efthymios Kouloumpis, Theresa Wilson, and Johanna Moore. Twitter sentiment analysis: The good the bad and the omg! _ICWSM_, 11:pages 538-541, 2011. \r\n\r\n[6] Hassan Saif, Yulan He, and Harith Alani. Semantic sentiment analysis of twitter. In _The Semantic Web-ISWC 2012_, pages 508-524. Springer, 2012. \r\n\r\n[7] Isaac G Councill, Ryan McDonald, and Leonid Velikovich. What's great and what's not: learning to classify the scope of negation for improved sentiment analysis. In _Proceedings of the workshop on negation and speculation in natural language processing_, pages 51-59. Association for Computational Linguistics, 2010. \r\n\r\n[8] Ilia Smirnov. Overview of stemming algorithms. _Mechanical Translation_, 2008. \r\n\r\n[9] Martin F Porter. An algorithm for suffix stripping. _Program: electronic library and information systems_, 40(3):pages 211-218, 2006. \r\n\r\n[10] Balakrishnan Gokulakrishnan, P Priyanthan, T Ragavan, N Prasath, and A Perera. Opinion mining and sentiment analysis on a twitter data stream. In _Advances in ICT for Emerging Regions (ICTer), 2012 International Conference on. IEEE_, 2012. \r\n\r\n[11] John Ross Quinlan. _C4. 5: programs for machine learning_, volume 1\\. Morgan kaufmann, 1993. \r\n\r\n[12] Steven Bird, Ewan Klein, and Edward Loper. _Natural language processing with Python_. \" O'Reilly Media, Inc.\", 2009.\r\n\r\n  \r\n  \r\n\r\n* * *\r\n\r\nFile translated from TEX by [ TTH](http://hutchinson.belmont.ma.us/tth/),\r\nversion 4.03.  \r\nOn 30 Jul 2014, 01:16.\r\n\r\n","google":"","note":"Don't delete this file! It's used internally to help with page regeneration."}